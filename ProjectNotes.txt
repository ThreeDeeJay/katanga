Katanga (maybe release name is Screen3d, Scr33d) is the virtual 3D screen.

Any given 3D game running on NVidia or SBS will take the bits and put them into
the screen Quad in the Unity environment.

The performance inside the injected game must be as fast as possible to avoid
CPU hits that can be very costly.  
The performance inside the Unity display needs to be fast at least 90Hz, to 
avoid any VR lag.  This can be simple scene, and will be on different CPU cores.
Sync problems will not exist, because we'll copy bits from the game to IPC, which
will then be put to the Quad. Since the headset only draws every 1/90th second,
there should be no tearing, because it will always fetch the newest image.


The Katanga project is multi-component.

1) The Unity app itself, which draw in the VR headset.
2) The UnityNativePlugin, which is native C code called from Unity.
3) The destination Deviare plugin, which will be injected into the game.

The Unity app is x64 only, because there is no point in a x32 version, since VR 
requires x64.  The Deviare plugin will be x32 and x64 depending upon the game.

DX9 and DX11 and OpenGL should all be supported with the Deviare Plugin.


Trying to decide whether to use Deviare2 itself, or to use the in-proc version only.
Not completely clear what would be best, and Deviare2 is fairly confusing and
poorly documented.  Lots of funny pieces that are not clear, like their Agent, and
active plugins. Plugins can be hook specific and native.

The code for the Unity project itself must include a C++ chunk, because we need
to call the OpenVR code, which is native C++.  Maybe this is handled by the Unity
plugin code, as we enable VR and it activates, but we still need access to the 
buffer for the quad/screen.

We don't need to hook remotely, hooking from inside the running game is OK.  And we
really only need the Present() call.  There also does not seem to be a particularly
good callback mechanism. They have OnFunctionCalled, but that is before we finish
copying the backbuffer, and we need a more direct IPC of some form.

Probably we need to use memory mapped file as the IPC, so that the backbuffer copy to
the Unity app is fast.  We can also use that for notification of new data.

So, all in all, doesn't seem worth using Deviare over in-proc, but maybe.


Back with more thoughts.  Given that the Quad we are drawing into is managed by Unity
it seems to push toward using Deviare2 directly, not in-proc.  Unity is all C#, and 
it is harder to do in native plugin.

So, for starting at least, current plan is to not use native plugin, and do all the 
work in C#, including Deviare, and hooking the Present call.

The actual code for the game itself will be a Deviare native plugin in C++, that will
do the work of fetching the backbuffer and copying it for the Unity app to use.  That
code needs to be as fast as possible, to avoid impacting the game's CPU use.  


Starting down this path, ran into some weird Unity problems.  Was getting a missing file
error for the use of Nektra.Deviare2.dll, missing a stdole file that was 1.1 version.  
Not sure I understood all that, because the file exists in the right spots on Win10, but
was not being seen.  Making a copy from the interop files in Program Files(x86) and
dropping that into the Unity root worked to solve that error.  Doesn't seem releated to
Nektra, seems related to Unity.

Using the Nektra.Deviare2.dll from the 2.8.3 binary release.  I was able to build my own
nektra libs and changed params from XP support, but seems to be unnecessary to do all that.

Also required- regsrvr32 DeviareCOM64.dll.  Unity runs in x64 now, and the creation of
the NktSpyMgr was crashing with a COM exception because it was not registered.  This is 
possibly problematic, because we'll need this on target systems.

With those in place, the Unity C# code calling to NktSpyMgr actually works.

Right now it's setup for VS2017, can move back to VS2013 if that seems better.  Right at 
the moment, I'm going to keep it this way, because the Unity->VS debugging is already
setup and working for breakpoints.


Deviare CTest is a good example of similar behavior.  Uses the built in Agent, not needing
the custom plugin until performance is needed. Patches all hooks, works in VS, including
debug through their code.

Requires several pieces that were not obvious.  Both Deviaredb and db64.  DeviareCom64.dll 
because it's a 64 bit app.  dvagent.dll for x32 apps, dvagent64.dll for x64.


10-9-17
Mostly working in a basic format.  All hooking is working into game.  

App to built in Unity needs to be DX11 target and x64 only.  Oculus Rift requires that, 
so there is no point in making any conversions to other APIs.  

Basic strategy will be to create our game snapshot texture as a DX9EX Texture2D, because
that can share with DX11 easily, and also with DX9 easily. Works both way, so DX9 game
works, DX11 game works.  Destination is a DX11 Texture2D in Unity.

Testing a bare minimum Unity project seemed to reqired 10% of the GTX 980 GPU, when in 
medium quality mode.  On High quality, it was 22%.  There is some complexity in terms of
vsync and driver defaults, so best bet will be to profile it when ready.

Sticking with the DX9 hooking for now, because it's already working, and needs to be done
in any case.  DX11 is more familiar, but requires DXGI::Present hook instead.  However,
the GameSurface will be a DX9EX variant, in order to be shareable.

The destination copy texture must be a RenderTarget, because of driver restrictions. It
cannot be a simple surface.  Also, for sharing, it needs to be a RenderTarget because the
backbuffer is always a RenderTarget and they must match in order to share.
https://msdn.microsoft.com/en-us/library/windows/desktop/bb174471(v=vs.85).aspx


10-17-17
OK, turns out that the MSDN docs half-lie and it is not possible to use surface
sharing on any DX9 device.  Only DX9Ex devices will work.  Confirmation here:
https://www.gamedev.net/forums/topic/638495-shared-resources-eg-textures-between-devicesthreads/
And, also confirmed using the DX9 debug layer on Win7 setup.  When trying to use any
variant of DX9 and a surface return handle, it would put up the error:
Direct3D9: (ERROR) :Device is not capable of sharing resource. CreateTexture fails.

Using the sample program, I confirmed that using the DX9Ex path works without any
error or notice using Debug layer.  Can also use CreateRenderTarget, no Ex, but that
seems of little value.  The errors looked like a bad parameter to CreateRenderTarget,
but in fact it was incorrect device.

This also points out a really important conclusion.  The DX9 and DX9Ex objects are 
not in fact the same.  Coercing from an Ex object will work, because it's a superset,
but going from DX9 to DX9Ex does not work, because the fundamental object is different.
Also confirmed by using QueryInterface, and getting E-NOINTERFACE.


10-19-17
Finally figured out what the crash was about.  Fuck! I say.  Fucking Microsoft has a
bad dxd9.h header file, been bad for over 5 years.  The dx9Ex interface is missing a
routine name in the IDirect3D9Ex interface section, of RegisterSoftwareDevice. That 
makes the C interface for IDirect3D9Ex, off by one.  So it calls the wrong routine.
Like GetAdapterMonitor instead of CreateDeviceEx. Fuck! Cost me two weeks on this one.


10-20-17
Experimenting with Console app, which just does CreateDX9, then CreateDevice, to see
what all needs to be an Ex object.  When CreateDeviceEx is used, we end up getting
a debug layer break in CreateTexture that the pool cannot be managed. Which leads
down a path of tweaking parameters.
However, when trying this to create a shared surface:
	hr = pDev9->CreateRenderTarget(1280, 720, D3DFMT_A8R8G8B8, D3DMULTISAMPLE_NONE, 0, false,
		&gGameSurface, &gGameSurfaceShare);
we get error:
Direct3D9: (ERROR) :Device is not capable of sharing resource. CreateRenderTarget/CreateDepthStencil fails.

Clearly the runtime requires an IDirect3DDevice9Ex.
And, since you cannot create a IDirect3DDevice9Ex with a DX9 factory, that means the 
top level CreateD39 must be Ex as well.  

And, in test app, if I do StretchRect from dev9 to dev9Ex, I get:
Direct3D9: (ERROR) :DstSurface was not allocated with this Device. StretchRect fails.
Which clearly indicates that we cannot go cross-device.  If the source object is the
dev9Ex, then it just crashes, debug layer is probably lame for Ex.


10-24-17
Getting closer.  With it not successfully creating and copying into a shared surface,
we now need to display this in Unity app.  The access to the surface there is not
terrific, and we also need to have it switch from DX9 to DX11 for the Unity display,
so the plan is to create a C++ unit for Unity side, and use their plugin model to
get access to C++ in this app as well.  Mostly this just needs to use the HANDLE to
the shared surface, and copy the bits into the in-Unity TV screen. 
Some questions about what needs to be write-only for performance, and how to sync
the copies, but let's get it limping first.

At present using the simple DX9 test app of Textures, because when I launch TheBall,
it fires some debug layer problems like CreateTexture needing to use only defaultpool.
Probably will need to hook and patch up some of these since we are putting the game
into an DX9Ex runtime, which changes some usage.

Restructering the project folder layout, because we need to have the next piece of
the UnityNativePlugin for C++ access.  Also, this layout was never right, Unity
rebuilds the .sln file for their piece at every open, so it's better if it's by 
itself and used as a subproject to a main project.   Doing this piecemeal to avoid
breaking the repo.


11-8-17
Bit of a ballbuster here, with Deviare crashing at launch instead of returning an error. 
This was happening on a full build, instead of while running in Unity. The problem wound
up being that the DeviairePlugin DLL path would change from Unity to the compiled app, so
yeah, thanks Unity for that kick in the nuts too.  There is no /Assets when the app is
compiled, which is fucking retarded.  

Then there is this crash in Deviare, which happens if the DLL is not found.  This is also
completely lame.  If it can't find it, it should return an error, not blow up.  If you see
this crash, it's because of the missing deviare plugin dll.

Crash:
	>	00000001()	Unknown
 	[Frames below may be incorrect and/or missing]	
 	DvAgent.dll!TNktArrayList<CNktDvParam *,128,TNktArrayListItemRemove_Release<CNktDvParam *> >::RemoveAllElements() Line 330	C++
 	DvAgent.dll!CNktDvHookEngine::Hook(CNktDvHookEngine::tagHOOKINFO * aHookInfo=0x8007007e, unsigned long nCount=1, int bIsInternal=0) Line 494	C++
 	DvAgent.dll!CDvAgentMgr::OnEngMsg_AddHook(tagNKT_DV_TMSG_ADDHOOK * lpMsg=0xffe16054, CNktDvTransportBigData * lpConnBigData=0xffe300e0) Line 2522	C++
 	DvAgent.dll!CDvAgentMgr::TAC_OnEngineMessage(CNktDvTransportAgent * lpTransport=0x77ad4060, tagNKT_DV_TMSG_COMMON * lpMsg=0xffe16054, unsigned long nMsgSize=1084, CNktDvTransportBigData * lpConnBigData=0xffe300e0) Line 699	C++
 	DvAgent.dll!CNktDvTransportAgent::WorkerThreadProc(unsigned long nIndex=5) Line 564	C++
 	DvAgent.dll!TNktClassWorkerThread<CNktDvTransportAgent>::ThreadProc() Line 169	C++
 	DvAgent.dll!thread_start<unsigned int (__stdcall*)(void *)>(void * const parameter=0xffe0418c) Line 115	C++
 	kernel32.dll!@BaseThreadInitThunk@12()	Unknown
 	ntdll.dll!___RtlUserThreadStart@8()	Unknown
 	ntdll.dll!__RtlUserThreadStart@8()	Unknown

Once all that is straight, and we use the Application.dataPath to get the proper path 
to the /Assets or /appname_Data folder, then we can properly build the full path to the 
DeviarePlugin.dll, and pass it to LoadCustomDLL.  Using the forward slashes returned
from Unity seems to work without any trouble.


11-15-17
Performance with OculusVR as the type XR Setting. Headset sensor activated.
Using NVidia Inspector graph. TheBall splash screen.  Anaglyph 3D Vision.
 Oculus: 41-45% of GPU
 OpenVR: 52-55% of GPU
 None:   24-26% of GPU
 
No game running, no SBS script, simplest scene.
 Oculus: 13-14% of GPU
 OpenVR: 20-21% of GPU
 
So, Oculus for SDK is clearly superior for performance.  The Stats part of Unity is misleading,
because for Oculus it shows full frame rate of 1/90 ms.  For OpenVR, it shows 0.5ms for Unity code. 
Different ways they handle end of frame.
It's also notable how much impact this has.  
For GTX 980, we jump from 25% for the game, to 45% for game+Rift.

LegacyShader/Diffuse, the default, is 2% GPU faster than Standard. 12-13% GPU.
Using sbsShader did not help any.

As another test, to see how much overhead Unity provides, running the Oculus sample
app, TinyRoom, release x64.  
 Oculus TinyRoom: 12%-13% of GPU.  
 
That app will use Oculus SDK directly, with native DX11 calls. No reason to 
think we can ever do better than that.


11-25-17
Seriously hard to get all this working, but sucess!  Now getting stereo bits from the
game, into Unity as a texture.  The display texture is side-by-side.  Took much time
and debugging to figure out how to get stereo bits out of DX9, no samples, bad docs.
Also that target surface for StretchRect cannot be shared, or it breaks the stereo.

Every stage of the pipeline has been unbelievably complicated to get working.  It's 
not a surprise no one else has tried this.  


11-28-17
Finally!  Got stereo bits from the game, all the way to the headset, and showing in
stereo in the virtual 3D TV.  Yes! 

Last trick here was in Unity, trying to sweettalk the VR support into showing my SBS
image as actual stereo, half for each eye.  Currently not quite right, it's requiring
multipath support, and I really want SinglePass Stereo for performance.  But, this is
working, with multipath, and an OnPreRender call for the camera script.  That gets
called once for each eye, so I can alternate eyes for the CopyTexture call.

Using CopyTexture seems superior to Blit, because Blit requires a shader to run, and
CopyTexture just does something closer to a memcpy.  May not matter at all, because
ultimately something probably runs a shader to put bits into the quad, but I think
this might save a Draw call from shaders.  Probably does not matter, not measurable.

Performance with a full Release buid is good.  Game runs well, if it's full screen
it's at full 60 fps. No F notification in VR, very smooth.  55-70% of GPU in use.
Some stalls during flyover in game.  


11-30-17
And... Winner!  Got it to show stereo all the way to the virtual 3D TV.  Correct eyes,
changeable depth and convergence like you'd want.  Looks good.

The key aspect here was working out the best way to do this.  The best way seems to
be to have a custom shader attached the the Quad Material, which does the work of 
copying the texture to the actual VR screen and transforming them through the MVP
matrix, as the head camera moves.  This saves an extra copy of the bits, and that
stage has to run anyway, so modifying that shader seems like best.  While there,
it has the ability to use the primary input texture of _bothEyes, and decide to
fetch either left or right half depending upon the unity_StereoEyeIndex variable.
Changes which half of the texture bits are fetched from, and thus shows the correct
piece to the correct eye.

We specifically do not use the Graphics.Blit, because that required a script on the
camera in order to function, and it wasn't clear if would work to handle both eyes.
It would also be another copy that is unnecessary.
Similarly, we don't Graphics.CopyTexture to the Quad Material, because there needed
to be two copies, and had to be done from the camera script as well, where the
OnPreRender would get called twice, so we could swap eyes.  Also a second copy, 
although this does work when using Multi-Pass Stereo.  In SinglePassStereo, OnPreRender
is only called once, so this won't work for the faster path.  
Tried to work out how to use the vrDesc for a RenderTexture, but nothing made
sense, and could not get anything to work.  It was documented as automatically
working for built in shaders, but it never seemed to recognize it was a VR texture.

This is also using SinglePassStereo.  This is supposed to be quite a lot faster,
and we don't need multipass.  Successfully drawing in VR using SinglePassStereo, using
the custom shader.

This is going to be nearly as fast as it is possible to make it.  The bits from the 
game are copied only twice, once to get stereo bits cleanly out of game, then once
again to the shared texture.  The shared texture is used directly in the DX11 side,
by the shader to fetch pixels.  So, very little extra copies.  Would be nice to lose
that second copy, but NVidia doesn't work copying to a shared resource.

Overall performance is good. GPU usage is fairly low, 38-42% GPU on the splash screen.
There are some F frame rate indicators in VR that are a concern.  Probably blocking
VR from multithreaded access causing stalls, but don't really know.  Need to look.
Ths is using 3D Vision to screen, which might be locked to 60Hz, and stall when
lower.  We get stalls during Ball flyover.  Oculus F indicator up constantly when
the ball is close with pull effect.  With that effect on, GPU usage goes to 70-74%.
Idling in-game is 57-60%.

No crashes.  Very solid.


12-3-17
Looking at a dropped Compositor problem, where I see C show up periodically.  Should
not be happening, we have maybe 50% of GPU headroom.  Tried backing up from the
extra thread to do second copy, and that did not help, so extra thread is not the
problem.  
Lots of other tests here, cannot quite pin it down.  Almost seems like a false
error, although Oculus debug tends to be very good.  No other tool shows a snag
when the ball is transparent up close, where in headset we see flashing F.  Tried
VS profile, Unity GPU profile.  And multiple paths of dropping pieces of my 
pipeline.  Only change that mattered was drawing the shared surface, even if bits
were not changing.
If not in the full screen foreground, I get Fs.  If it's full screen, but not front
app, still get frames through, but no Fs.
Changing vsync in game, to limited to 160 using .ini.  Works.  Game runs at 105 fps
natively.  Still getting Fs.

Turning off shared surface altogether, and setting the Textures to null still gives
me Fs.  So even with no share at all between apps, it still stalls.  This suggests
that it has something to do with the GPU itself, where the dual GPU processes stall
the pipeline, or can't switch effectively or something.
Definitely seems related to the transparency effect.  When I set ball to not transparent
it doesn't F, in windowed mode.  Game specific?  Might be pipeline flush on transparent
or something.

After doing quite a bit of analysis, including using GPUView to look at GPU usage while
both apps are running- the problem is that TheBall has a sequence that takes 8ms of time
and locks out anything else from getting GPU time during that sequence. When this overlaps
the Present for Katanga, it stalls and shows a dropped F frame. 
The actual underlying problem is that the GPU scheduler is retarded, and apparently 
cannot be tuned.  It decides unilatterally that the frontmost window TheBall is more 
important than anything else and thus does not give up time when our much more critical
VR app calls.  SetGPUPriority does nothing, and there are no nvapi calls that would
allow a fix. Scheduler is a black box and fuck off.


12-6-17
Interesting experiment where I only create a DX9Ex factory, but then allow the calls
to go through as normal DX9 calls for CreateDevice, CreateTexture, Present et. al.
Creating the Shared resource still works, no error, and I get stereo out.
This is probably the best way to go, to avoid having to tweak all the other calls for
DX9, like CreateTexture, CreateTexure3D to add parameters.
This works with only DX9Ex factory, but the returned CreateDevice is still considered
a Device9Ex by the debug runtime, and I still need all the CreateTexture/Buffer overrides
to fix those debug flaws.


5-18-18
Pretty big gap there as I lost motivation.  Back looking at the dropped frames problem.
Definitely seems to be priority related, but I have no tools to fix that. Asked for
access to Context Priority in the NVidia VRworks, but no one responds, including Dave.

Managed to create a semaphore based stall system, using a Windows Event that can be
triggered on/off.  In the main VR app, I trigger this off after 3ms from the front of
the frame, then in the game, at patched draw calls, I look for and stall if that is
off.  This works in that the code does what I expected, and doesn't seem to have any
bugs.  In GPUView I can see that the game 8ms blob is broken into two pieces and the
frame rate in the game drops to 45.  

Not a full solution though.  Still get occasional dropped frames as it conflicts.
It would be possible to tune it for just this game, but that doesn't seem like a 
particularly hot strategy anyway.  

Switching now to embedding the DX11 VR side into the game app as well, and doing the 
entire VR world on my own handbuilt code, no Unity.  This gives me the most flexibility,
with the most work as well.  Worth an experiment to see if I can keep performance up
this way.  Experiments with VorpX suggest that he is doing this, and his performance 
seems spot on.

Still going to use Unity app for the moment, as all the launching and Deviare injection
happen there.  Just turning off the VR aspect, so as far as it is concerned, it's just
a regular 2D game.


8-5-18
Installing for new Squanchando computer.  Decided to update the VS2017 and latest LTS
Unity.  Reasoning that it's better to have latest graphical debugging tools here, as
more valuable than stable setup.  Causes a few problems, like broken directory paths.

Also of note, doesn't work at all on WMR, because WMR does not presently support 
3D Vision.  Driver crashes if 3D is enabled.  Bug reported.


8-27-18
Tried to get Registration-Free COM to work, but cannot quite get the right combo for
the Unity app.  Too strange a runtime, not clear where to connect.  Got it working
OK for InvisibleWalls sample, including a subdirectory of DeviareCOM, but nothing 
else.  Does not appear to support an arbitrary directory. Documentation sucks, and I
could not find any examples of people using a different directory than the root.
Even using the root for Unity does not work, probably because they set the working
directory to PlugIns or something.  Super fragile mechanism, no good debug tools.
The sxstrace command creates an empty log in the Unity case.  All in all, a good
waste of multiple weeks.

Change of plan there- RegFree is a nice-to-have, not required.  Definitely far 
superior, but tweaky and hard to figure out.  Easy enough to regsvr32 at install 
time instead, so let's skip the distraction and get on more important stuff.

Like avoiding the Steam sublaunch for hooking.  If I follow:
https://stackoverflow.com/questions/9624629/debug-games-from-steam-with-pix
and create a steamapp_id.txt file with 35460 in it, I can successfully launch
the Steam version of TheBall without any problems.